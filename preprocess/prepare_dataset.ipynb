{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = \"/Volumes/X/Data/fusion-s1-s2/\"\n",
    "S2_ROOT_PATH = f\"{DATA_ROOT_DIR}s2/sre-10m/\"\n",
    "ORBIT = \"044\"\n",
    "S1_ROOT_PATH = f\"{DATA_ROOT_DIR}s1db/32VNH/threeband/{ORBIT}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/candidates_cloudy.json\", \"r\") as f:\n",
    "    candidates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_date(target_date, date_array):\n",
    "    target = datetime.strptime(target_date, '%Y%m%d')\n",
    "    date_array = [d for d in date_array if \"Store\" not in d]\n",
    "    date_array = [datetime.strptime(date, '%Y%m%d') for date in date_array]\n",
    "    closest_date = min(date_array, key=lambda x: abs(target - x))\n",
    "    return closest_date.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN, DATASET_TEST, DATASET_VALIDATION, DATASET = {}, {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_dates = [d.split(\"_\")[-1] for d in os.listdir(f\"data/cropped/s1/\")]\n",
    "idx = 0\n",
    "\n",
    "for k, v in candidates.items():\n",
    "    cloudy = v[\"cloudy\"]\n",
    "    cloudy_name = \"_\".join(v[\"cloudy\"].split(\"_\")[:3])\n",
    "\n",
    "    cloud_free = v[\"cloud_free\"]\n",
    "    cloud_free_name = \"_\".join(v[\"cloud_free\"].split(\"_\")[:3])\n",
    "    date_cloudy = cloudy.split(\"_\")[2]\n",
    "\n",
    "    s1_date = closest_date(date_cloudy, s1_dates)\n",
    "\n",
    "    TEMP_DATASET = {\n",
    "        \"s2_cloudy_B02\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B02/{'_'.join(cloudy.split('_')[:3])}_B02_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B03\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B03/{'_'.join(cloudy.split('_')[:3])}_B03_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B04\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B04/{'_'.join(cloudy.split('_')[:3])}_B04_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        # \"s2_cloudy_B05\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B05/{'_'.join(cloudy.split('_')[:3])}_B05_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        # \"s2_cloudy_B06\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B06/{'_'.join(cloudy.split('_')[:3])}_B06_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        # \"s2_cloudy_B07\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B07/{'_'.join(cloudy.split('_')[:3])}_B07_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B08\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B08/{'_'.join(cloudy.split('_')[:3])}_B08_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        # \"s2_cloudy_B8A\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B8A/{'_'.join(cloudy.split('_')[:3])}_B8A_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        # \"s2_cloudy_B11\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B11/{'_'.join(cloudy.split('_')[:3])}_B11_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        # \"s2_cloudy_B12\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B12/{'_'.join(cloudy.split('_')[:3])}_B12_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B02\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B02/{'_'.join(cloud_free.split('_')[:3])}_B02_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B03\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B03/{'_'.join(cloud_free.split('_')[:3])}_B03_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B04\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B04/{'_'.join(cloud_free.split('_')[:3])}_B04_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        # \"s2_cloud_free_B05\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B05/{'_'.join(cloud_free.split('_')[:3])}_B05_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        # \"s2_cloud_free_B06\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B06/{'_'.join(cloud_free.split('_')[:3])}_B06_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        # \"s2_cloud_free_B07\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B07/{'_'.join(cloud_free.split('_')[:3])}_B07_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B08\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B08/{'_'.join(cloud_free.split('_')[:3])}_B08_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        # \"s2_cloud_free_B8A\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B8A/{'_'.join(cloud_free.split('_')[:3])}_B8A_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        # \"s2_cloud_free_B11\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B11/{'_'.join(cloud_free.split('_')[:3])}_B11_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        # \"s2_cloud_free_B12\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B12/{'_'.join(cloud_free.split('_')[:3])}_B12_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s1_hv\": f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{s1_date}_HV/S1_32VNH_{s1_date}_HV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s1_vv\": f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{s1_date}_VV/S1_32VNH_{s1_date}_VV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "    }\n",
    "\n",
    "    if all([os.path.isfile(tv) for tv in TEMP_DATASET.values()]):\n",
    "        DATASET[idx] = TEMP_DATASET\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_cloudy.json\", \"w\") as f:\n",
    "    json.dump(DATASET, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_REMOVE_SHADOW_ON_GT = [0,5,7,8,18,84,141,146,148,149,158,159,161,177,250,493,503,564,577,579,\n",
    "                          584,685,686,687,737,743,745,746,748,913,941,950,966,973,1195,1200,1201,1215,1274,1285,1294,1305,1311,\n",
    "                          1489,1490,1493,1502,1534,1541,1544]\n",
    "TO_REMOVE_WRONG_GT = [24,53,54,55,57,58,60,64,65,68,71,79,171,172,173,174,175,176,260,\n",
    "                      262,270,271,272,273,397,643,677,690,712,1078,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,\n",
    "                      1260,1320,1474,1475]\n",
    "TO_REMOVE_WEIRD_THINGS_ON_GT = [1,185]\n",
    "TO_REMOVE_CLOUDS_ON_GT = [4,11,156,256,259,268,269,274,275,276,301,319,326,334,478,491,539,549,622,753,759,883,902,\n",
    "                          911,942,1004,1159,1161,1219,1220,1223,1224,1414,1420,1482,1585]\n",
    "TO_REMOVE = TO_REMOVE_SHADOW_ON_GT + TO_REMOVE_WRONG_GT + TO_REMOVE_WEIRD_THINGS_ON_GT + TO_REMOVE_CLOUDS_ON_GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/to_remove.txt\", \"a\") as f:\n",
    "    for i in TO_REMOVE:\n",
    "        f.write(str(DATASET[str(i)][\"s2_cloud_free_B02\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in TO_REMOVE:\n",
    "    del DATASET[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_cloudy.json\", \"w\") as f:\n",
    "    json.dump(DATASET, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset\n",
    "with open(\"data/dataset_cloudy.json\", \"r\") as f:\n",
    "    DATASET = json.load(f)\n",
    "    \n",
    "dataset_keys = list(DATASET.keys())\n",
    "np.random.shuffle(dataset_keys)\n",
    "split_index_80 = int(0.8 * len(dataset_keys))\n",
    "split_index_80 = int(0.9 * len(dataset_keys))\n",
    "\n",
    "dataset_keys_train = dataset_keys[:split_index_80]\n",
    "dataset_keys_test = dataset_keys[split_index_80:split_index_80]\n",
    "dataset_keys_validation = dataset_keys[split_index_80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k in dataset_keys_train:\n",
    "    DATASET_TRAIN[i] = DATASET[k]\n",
    "    i += 1\n",
    "\n",
    "with open(\"data/dataset_cloudy_train.json\", \"w\") as f:\n",
    "    json.dump(DATASET_TRAIN, f)\n",
    "\n",
    "l = 0\n",
    "for k in dataset_keys_test:\n",
    "    DATASET_TEST[l] = DATASET[k]\n",
    "    l += 1\n",
    "with open(\"data/dataset_cloudy_test.json\", \"w\") as f:\n",
    "    json.dump(DATASET_TEST, f)\n",
    "\n",
    "d = 0\n",
    "for k in dataset_keys_validation:\n",
    "    DATASET_TEST[d] = DATASET[k]\n",
    "    d += 1\n",
    "with open(\"data/dataset_cloudy_validation.json\", \"w\") as f:\n",
    "    json.dump(DATASET_TEST, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_cloudy_test.json\", \"r\") as f:\n",
    "    DATASET = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in DATASET.items():\n",
    "    print(k)\n",
    "    cB2 = gdal.Open(f\"{v['s2_cloudy_B02']}\").ReadAsArray()\n",
    "    cB3 = gdal.Open(f\"{v['s2_cloudy_B03']}\").ReadAsArray()\n",
    "    cB4 = gdal.Open(f\"{v['s2_cloudy_B04']}\").ReadAsArray()\n",
    "\n",
    "    c = np.stack((cB4, cB3, cB2), axis=-1)/2000\n",
    "\n",
    "    gB2 = gdal.Open(f\"{v['s2_cloud_free_B02']}\").ReadAsArray()\n",
    "    gB3 = gdal.Open(f\"{v['s2_cloud_free_B03']}\").ReadAsArray()\n",
    "    gB4 = gdal.Open(f\"{v['s2_cloud_free_B04']}\").ReadAsArray()\n",
    "\n",
    "    g = np.stack((gB4, gB3, gB2), axis=-1)/2000\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # Create the first subplot, add title, and display the first image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"input\")\n",
    "    plt.imshow(c)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Create the second subplot, add title, and display the second image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"truth\")\n",
    "    plt.imshow(g)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save the figure to a file\n",
    "    print(os.getcwd())\n",
    "\n",
    "    plt.savefig(f\"filtering/{k}_{v['s2_cloudy_B02'].split('.')[0].split('/')[-1]}.png\")\n",
    "\n",
    "    # Close the figure\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
