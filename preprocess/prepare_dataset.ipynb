{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = \"/Volumes/X/Data/fusion-s1-s2/\"\n",
    "S2_ROOT_PATH = f\"{DATA_ROOT_DIR}s2/sre-10m/\"\n",
    "ORBIT = \"044\"\n",
    "S1_ROOT_PATH = f\"{DATA_ROOT_DIR}s1db/32VNH/threeband/{ORBIT}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_date(target_date, date_array):\n",
    "    target = datetime.strptime(target_date, '%Y%m%d')\n",
    "    date_array = [datetime.strptime(date, '%Y%m%d') for date in date_array]\n",
    "    closest_date = min(date_array, key=lambda x: abs(target - x))\n",
    "    return closest_date.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_difference(date1, date2):\n",
    "    d1 = datetime.strptime(date1, '%Y%m%d')\n",
    "    d2 = datetime.strptime(date2, '%Y%m%d')\n",
    "    difference = abs(d1 - d2)\n",
    "\n",
    "    # Convert the difference to 'YYYYMMDD' format\n",
    "    years = difference.days // 365\n",
    "    months = (difference.days % 365) // 30\n",
    "    days = (difference.days % 365) % 30\n",
    "\n",
    "    return f'{years:04d}{months:02d}{days:02d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/candidates_filtered_water_MAXED.json\", \"r\") as f:\n",
    "    candidates = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_date(target_date, date_array):\n",
    "    target = datetime.strptime(target_date, '%Y%m%d')\n",
    "    date_array = [d for d in date_array if \"Store\" not in d]\n",
    "    date_array = [datetime.strptime(date, '%Y%m%d') for date in date_array]\n",
    "    closest_date = min(date_array, key=lambda x: abs(target - x))\n",
    "    return closest_date.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_date_before(target_date, date_array):\n",
    "    target = datetime.strptime(target_date, '%Y%m%d')\n",
    "    date_array = [d for d in date_array if \"Store\" not in d]\n",
    "    date_array = [datetime.strptime(date, '%Y%m%d') for date in date_array]\n",
    "    before_target = [date for date in date_array if date <= target]\n",
    "    try:\n",
    "        closest_date = max(before_target)\n",
    "    except:\n",
    "        closest_date = target\n",
    "    return closest_date.strftime('%Y%m%d')\n",
    "\n",
    "def closest_date_after(target_date, date_array):\n",
    "    target = datetime.strptime(target_date, '%Y%m%d')\n",
    "    date_array = [d for d in date_array if \"Store\" not in d]\n",
    "    date_array = [datetime.strptime(date, '%Y%m%d') for date in date_array]\n",
    "    after_target = [date for date in date_array if date > target]\n",
    "    try:\n",
    "        closest_date = min(after_target)\n",
    "    except:\n",
    "        closest_date = target\n",
    "    return closest_date.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN, DATASET_TEST, DATASET = {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_dates = [d.split(\"_\")[-1] for d in os.listdir(f\"data/cropped/s1/\")]\n",
    "idx = 0\n",
    "\n",
    "for k, v in candidates.items():\n",
    "    cloudy = v[\"cloudy\"]\n",
    "    cloudy_name = \"_\".join(v[\"cloudy\"].split(\"_\")[:3])\n",
    "\n",
    "    cloud_free = v[\"cloud_free\"]\n",
    "    cloud_free_name = \"_\".join(v[\"cloud_free\"].split(\"_\")[:3])\n",
    "    date_cloudy = cloudy.split(\"_\")[2]\n",
    "\n",
    "    s1_date = closest_date(date_cloudy, s1_dates)\n",
    "    after_date = closest_date_after(date_cloudy, s1_dates)\n",
    "    before_date = closest_date_before(date_cloudy, s1_dates)\n",
    "\n",
    "    TEMP_DATASET = {\n",
    "        \"s2_cloudy_B02\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B02/{'_'.join(cloudy.split('_')[:3])}_B02_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B03\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B03/{'_'.join(cloudy.split('_')[:3])}_B03_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B04\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B04/{'_'.join(cloudy.split('_')[:3])}_B04_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B05\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B05/{'_'.join(cloudy.split('_')[:3])}_B05_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B06\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B06/{'_'.join(cloudy.split('_')[:3])}_B06_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B07\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B07/{'_'.join(cloudy.split('_')[:3])}_B07_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B08\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B08/{'_'.join(cloudy.split('_')[:3])}_B08_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B8A\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B8A/{'_'.join(cloudy.split('_')[:3])}_B8A_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B11\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B11/{'_'.join(cloudy.split('_')[:3])}_B11_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloudy_B12\": f\"data/cropped/s2/{cloudy_name}/{cloudy_name}_B12/{'_'.join(cloudy.split('_')[:3])}_B12_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B02\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B02/{'_'.join(cloud_free.split('_')[:3])}_B02_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B03\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B03/{'_'.join(cloud_free.split('_')[:3])}_B03_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B04\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B04/{'_'.join(cloud_free.split('_')[:3])}_B04_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B05\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B05/{'_'.join(cloud_free.split('_')[:3])}_B05_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B06\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B06/{'_'.join(cloud_free.split('_')[:3])}_B06_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B07\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B07/{'_'.join(cloud_free.split('_')[:3])}_B07_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B08\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B08/{'_'.join(cloud_free.split('_')[:3])}_B08_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B8A\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B8A/{'_'.join(cloud_free.split('_')[:3])}_B8A_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B11\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B11/{'_'.join(cloud_free.split('_')[:3])}_B11_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s2_cloud_free_B12\": f\"data/cropped/s2/{cloud_free_name}/{cloud_free_name}_B12/{'_'.join(cloud_free.split('_')[:3])}_B12_{'_'.join(cloud_free.split('_')[3:])}\",\n",
    "        \"s1_hv\": f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{s1_date}_HV/S1_32VNH_{s1_date}_HV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s1_vv\": f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{s1_date}_VV/S1_32VNH_{s1_date}_VV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s1_hv_-1\":f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{before_date}_HV/S1_32VNH_{before_date}_HV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s1_hv_+1\":f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{after_date}_HV/S1_32VNH_{after_date}_HV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s1_vv_-1\":f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{before_date}_HV/S1_32VNH_{before_date}_HV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "        \"s1_vv_+1\":f\"data/cropped/s1/S1_32VNH_{s1_date}/S1_32VNH_{after_date}_HV/S1_32VNH_{after_date}_HV_{'_'.join(cloudy.split('_')[3:])}\",\n",
    "    }\n",
    "\n",
    "    if all([os.path.isfile(tv) for tv in TEMP_DATASET.values()]):\n",
    "        DATASET[idx] = TEMP_DATASET\n",
    "        idx += 1\n",
    "\n",
    "with open(\"data/dataset_filtered_water_timeseries.json\", \"w\") as f:\n",
    "    json.dump(DATASET, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset\n",
    "with open(\"data/dataset_filtered_water_train_filtered.json\", \"r\") as f:\n",
    "    DATASET = json.load(f)\n",
    "    \n",
    "dataset_keys = list(DATASET.keys())\n",
    "np.random.shuffle(dataset_keys)\n",
    "split_index = int(0.9 * len(dataset_keys))\n",
    "\n",
    "dataset_keys_train = dataset_keys[:split_index]\n",
    "dataset_keys_test = dataset_keys[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k in dataset_keys_train:\n",
    "    DATASET_TRAIN[i] = DATASET[k]\n",
    "    i += 1\n",
    "\n",
    "with open(\"data/dataset_filtered_water_filtered_train.json\", \"w\") as f:\n",
    "    json.dump(DATASET_TRAIN, f)\n",
    "\n",
    "l = 0\n",
    "for k in dataset_keys_test:\n",
    "    DATASET_TEST[l] = DATASET[k]\n",
    "    l += 1\n",
    "with open(\"data/dataset_filtered_water_filtered_test.json\", \"w\") as f:\n",
    "    json.dump(DATASET_TEST, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_filtered_water_train.json\", \"r\") as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dataset.items():\n",
    "    print(k)\n",
    "    cB2 = gdal.Open(f\"{v['s2_cloudy_B02']}\").ReadAsArray()\n",
    "    cB3 = gdal.Open(f\"{v['s2_cloudy_B03']}\").ReadAsArray()\n",
    "    cB4 = gdal.Open(f\"{v['s2_cloudy_B04']}\").ReadAsArray()\n",
    "\n",
    "    c = np.stack((cB4, cB3, cB2), axis=-1)/2000\n",
    "\n",
    "    gB2 = gdal.Open(f\"{v['s2_cloud_free_B02']}\").ReadAsArray()\n",
    "    gB3 = gdal.Open(f\"{v['s2_cloud_free_B03']}\").ReadAsArray()\n",
    "    gB4 = gdal.Open(f\"{v['s2_cloud_free_B04']}\").ReadAsArray()\n",
    "\n",
    "    g = np.stack((gB4, gB3, gB2), axis=-1)/2000\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # Create the first subplot, add title, and display the first image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"input\")\n",
    "    plt.imshow(c)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Create the second subplot, add title, and display the second image\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"truth\")\n",
    "    plt.imshow(g)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save the figure to a file\n",
    "    print(os.getcwd())\n",
    "\n",
    "    plt.savefig(f\"filtering/{k}_{v['s2_cloudy_B02'].split('.')[0].split('/')[-1]}.png\")\n",
    "\n",
    "    # Close the figure\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\n",
    "    \"14\", \"17\", \"50\", \"72\", \"88\", \"107\", \"116\", \"141\", \"186\", \"245\", \"275\", \"355\", \"357\", \"381\", \"421\", \"462\", \"526\", \"527\",\n",
    "    \"530\", \"534\", \"551\", \"557\", \"562\", \"576\", \"655\", \"656\", \"661\", \"704\", \"723\", \"726\", \"737\", \"744\", \"769\", \"781\",\n",
    "    \"881\", \"892\", \"916\", \"921\", \"946\", \"976\", \"1051\", \"1055\", \"1065\", \"1078\", \"1127\", \"1133\", \"1139\", \"1145\", \"1151\", \"1154\",\n",
    "    \"1159\", \"1182\", \"1183\", \"1186\", \"1205\", \"1216\", \"1232\", \"1252\", \"1257\", \"1267\", \"1294\", \"1299\", \"1301\", \"1318\", \"1348\",\n",
    "    \"1366\", \"1369\", \"1385\", \"1406\", \"1414\"\n",
    "\n",
    "]\n",
    "\n",
    "maybe = [\"20\", \"30\", \"36\", \"51\", \"52\", \"53\", \"57\", \"65\", \"73\", \"94\", \"118\", \"139\", \"150\", \"151\", \"163\", \"162\"\n",
    "         \"173\", \"186\", \"201\", \"830\", \"829\"]\n",
    "\n",
    "fin = [\"\"]\n",
    "moyen = [\"209\"]\n",
    "gros = []\n",
    "\n",
    "to_look_into = \"590_S2_32VNH_20190505_B02_952_515340_6241120_256 and 591_S2_32VNH_20200812_B02_952_515340_6241120_256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for id in to_remove:\n",
    "    path = dataset[id]\n",
    "    del dataset[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset_filtered_water_train_filtered.json\", \"w\") as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
