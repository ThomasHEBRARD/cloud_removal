{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Vg6ziqrfor3U"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization       \n",
        "from keras.layers import LeakyReLU\n",
        "from copy import copy, deepcopy\n",
        "from numpy import load\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randint\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tNGhxxHwqkXL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import re\n",
        "import pandas as pd\n",
        "from osgeo import gdal\n",
        "import os\n",
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from numpy import vstack\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from numpy import savez_compressed\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sSPcjdThSUgG"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = \"data/vrt/s2/rgb/\"\n",
        "DATA_PATH = \"/Volumes/X/Data/fusion-s1-s2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_crops(image, crop_height, crop_width, stride, num_bands):\n",
        "    cropped_images = []\n",
        "\n",
        "    for y in range(0, image.shape[0] - crop_height + 1, stride):\n",
        "        for x in range(0, image.shape[1] - crop_width + 1, stride):\n",
        "            cropped_image = image[y:y + crop_height, x:x + crop_width, :num_bands]\n",
        "            cropped_images.append(cropped_image)\n",
        "\n",
        "    return np.array(cropped_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_and_preprocess_vrt(file_path, crop_height, crop_width, stride, crop=False):\n",
        "    # Read the image\n",
        "    img = gdal.Open(file_path)\n",
        "    bands = img.RasterCount\n",
        "    rows, cols = img.RasterYSize, img.RasterXSize\n",
        "    image = np.zeros((rows, cols, bands))\n",
        "\n",
        "    for band_index in range(bands):\n",
        "        bandx = img.GetRasterBand(band_index + 1)\n",
        "        datax = bandx.ReadAsArray()\n",
        "        image[:, :, band_index] = datax\n",
        "\n",
        "    # Create crops\n",
        "    if crop:\n",
        "        cropped_images = create_crops(image, crop_height, crop_width, stride, bands)\n",
        "        return cropped_images\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cloud_free_vrts = [\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190227.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190418.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190423.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190831.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200422.vrt\",\n",
        "    \"data/vrt/s2/translate/S2_32VNH_20200601.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200914.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20211029.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190418.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190423.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190831.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200323.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200422.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200601.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200914.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210616.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210909.vrt\"\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20211029.vrt\",\n",
        "]\n",
        "\n",
        "cloudy_vrts = [\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190113.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190309.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190503.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190513.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190602.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190617.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190811.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190821.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190905.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190915.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20190930.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20191020.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20191114.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20191209.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200103.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200113.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200313.vrt\",\n",
        "    \"data/vrt/s2/translate/S2_32VNH_20200611.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200711.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200830.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200904.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20200929.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20201029.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20201113.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210122.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210201.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210303.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210407.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210412.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210422.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210502.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210606.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20210914.vrt\",\n",
        "    # \"data/vrt/s2/translate/S2_32VNH_20211123.vrt\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "for path in cloud_free_vrts:\n",
        "    if not os.path.isfile(path):\n",
        "        print(path)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "data =  None\n",
        "with open(\"data/fmask_stats.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for k, v in data.items():\n",
        "    t = \"no_observation\" not in v\n",
        "    tt = \"no_observation\" in v and v[\"no_observation\"] < 1\n",
        "    if t or tt:\n",
        "        if \"cloud\" in v and v[\"cloud\"] > 50 and v[\"cloud\"] < 70:\n",
        "            print(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cloudy_images = [read_and_preprocess_vrt(file_path, 1024, 1024, stride=1024, crop=True) for file_path in cloudy_vrts]\n",
        "cloud_free_images = [read_and_preprocess_vrt(file_path, 1024, 1024, stride=1024, crop=True) for file_path in cloud_free_vrts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cloudy_images=cloudy_images[0]\n",
        "cloud_free_images=cloud_free_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_images(images):\n",
        "    # min_value = images.min()\n",
        "    # max_value = images.max()\n",
        "    # normalized_images = (images - min_value) / (max_value - min_value)\n",
        "    normalized_images = images/2000\n",
        "    return normalized_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalized_cloudy_images = [normalize_images(cloudy_image) for cloudy_image in cloudy_images]\n",
        "normalized_cloud_free_images = [normalize_images(cloud_free_image) for cloud_free_image in cloud_free_images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "for i in range(num_images):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Plot the cloud-free image\n",
        "    ax1.imshow(normalized_cloud_free_images[i])\n",
        "    ax1.set_title(f\"Cloud-free image {i+1}\")\n",
        "    ax1.axis(\"off\")\n",
        "\n",
        "    # Plot the corresponding cloudy image\n",
        "    ax2.imshow(normalized_cloudy_images[i])\n",
        "    ax2.set_title(f\"Cloudy image {i+1}\")\n",
        "    ax2.axis(\"off\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the ratios for the dataset split\n",
        "train_ratio = 0.8\n",
        "validation_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Split the data into training and the remaining sets\n",
        "X_train, X_remaining, y_train, y_remaining = train_test_split(\n",
        "    normalized_cloudy_images, normalized_cloud_free_images, test_size=(1 - train_ratio), random_state=42\n",
        ")\n",
        "\n",
        "# Calculate the size of the validation and testing sets\n",
        "remaining_size = validation_ratio + test_ratio\n",
        "validation_size = validation_ratio / remaining_size\n",
        "test_size = test_ratio / remaining_size\n",
        "\n",
        "# Split the remaining data into validation and testing sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_remaining, y_remaining, test_size=test_size, random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = normalized_cloud_free_images\n",
        "X_test = normalized_cloudy_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_images = 100\n",
        "\n",
        "for i in range(num_images):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Plot the cloud-free image\n",
        "    ax1.imshow(X_train[i])\n",
        "    ax1.set_title(f\"Cloud-free image {i+1}\")\n",
        "    ax1.axis(\"off\")\n",
        "\n",
        "    # Plot the corresponding cloudy image\n",
        "    ax2.imshow(X_test[i])\n",
        "    ax2.set_title(f\"Cloudy image {i+1}\")\n",
        "    ax2.axis(\"off\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "colab_type": "code",
        "id": "ZHSg4FCeWWVE",
        "outputId": "ee022a3b-57de-4a53-e291-f2921bcf2885"
      },
      "outputs": [],
      "source": [
        "#----------training stack-------------#\n",
        "tem1 = deepcopy(temp_train)\n",
        "# -------- MY NEW APPROACH -----------------#\n",
        "# #no skew\n",
        "# tem3 = deepcopy(temp_train)\n",
        "\n",
        "# #90 deg clockwise\n",
        "# tem4 = deepcopy(temp_train)\n",
        "# tem4 = np.rot90(tem4, axes=(1,0))\n",
        "\n",
        "# #90 deg anti-clockwise\n",
        "# tem5 = deepcopy(temp_train)\n",
        "# tem5 = np.rot90(tem5, axes=(0,1))\n",
        "\n",
        "# #180 deg \n",
        "# #rotate 180 deg clockwise/anti-clockwise\n",
        "# tem6 = deepcopy(temp_train)\n",
        "# tem6 = np.rot90(tem6, axes=(1,0))\n",
        "# tem6 = np.rot90(tem6, axes=(1,0))\n",
        "# ------------------------------------------#\n",
        "\n",
        "# ---------- REFERENCE CODE APPROACH --------#\n",
        "tem3=np.rot90(tem1)\n",
        "tem4=np.rot90(tem1, 2)\n",
        "tem5=np.rot90(tem1, 3)\n",
        "# -------------------------------------------#\n",
        "\n",
        "#displaying the distinct images of the training stack\n",
        "fig, a = plt.subplots(1,4, figsize=(16,4))\n",
        "\n",
        "a[0].imshow(tem1)\n",
        "a[0].set_title('original')\n",
        "a[0].axis('off')\n",
        "\n",
        "a[1].imshow(tem3)\n",
        "a[1].set_title('-90-deg')\n",
        "a[1].axis('off')\n",
        "\n",
        "a[2].imshow(tem4)\n",
        "a[2].set_title('180-deg')\n",
        "a[2].axis('off')\n",
        "\n",
        "a[3].imshow(tem5)\n",
        "a[3].set_title('+90-deg')\n",
        "a[3].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "2pZzS9QqXz5v",
        "outputId": "bbcec8bf-0139-40d1-85aa-2dc8a4d1d098"
      },
      "outputs": [],
      "source": [
        "#stacking the augmented TRUE images\n",
        "train_stack = np.stack((tem3, tem4, tem5, tem3, tem4, tem5, tem3, tem4, tem5, tem3), axis=0)\n",
        "print(train_stack.shape)\n",
        "\n",
        "#saving the training set on disk\n",
        "np.save(ROOT_PATH + '/data/train_data_3_10copy', train_stack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "colab_type": "code",
        "id": "xspv0EBJprgt",
        "outputId": "30a56d5f-f7f0-445e-d1dd-f2dda157dd57"
      },
      "outputs": [],
      "source": [
        "#----------testing stack-------------#\n",
        "tem2 = deepcopy(temp_test)\n",
        "# # -------- MY NEW APPROACH -----------------#\n",
        "# #no skew\n",
        "# tem7 = deepcopy(temp_test)\n",
        "\n",
        "# #90 deg clockwise\n",
        "# tem8 = deepcopy(temp_test)\n",
        "# tem8 = np.rot90(tem8, axes=(1,0))\n",
        "\n",
        "# #90 deg anti-clockwise\n",
        "# tem9 = deepcopy(temp_test)\n",
        "# tem9 = np.rot90(tem9, axes=(0,1))\n",
        "\n",
        "# #180 deg \n",
        "# #rotate 180 deg clockwise/anti-clockwise\n",
        "# tem10 = deepcopy(temp_test)\n",
        "# tem10 = np.rot90(tem10, axes=(1,0))\n",
        "# tem10 = np.rot90(tem10, axes=(1,0))\n",
        "# # ------------------------------------------#\n",
        "\n",
        "# ---------- REFERENCE CODE APPROACH --------#\n",
        "tem6=np.rot90(tem2)\n",
        "tem7=np.rot90(tem2, 2)\n",
        "tem8=np.rot90(tem2, 3)\n",
        "# -------------------------------------------#\n",
        "\n",
        "#displaying the distinct images of the training stack\n",
        "fig, a = plt.subplots(1,4, figsize=(16,4))\n",
        "\n",
        "\n",
        "a[0].imshow(tem2)\n",
        "a[0].set_title('original')\n",
        "a[0].axis('off')\n",
        "\n",
        "a[1].imshow(tem6)\n",
        "a[1].set_title('-90-deg')\n",
        "a[1].axis('off')\n",
        "\n",
        "a[2].imshow(tem7)\n",
        "a[2].set_title('180-deg')\n",
        "a[2].axis('off')\n",
        "\n",
        "a[3].imshow(tem8)\n",
        "a[3].set_title('+90-deg')\n",
        "a[3].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "DMg9PsYdqas7",
        "outputId": "3a002935-6d3c-4020-cd3a-255150fc495d"
      },
      "outputs": [],
      "source": [
        "#stacking the augmented FALSE images\n",
        "train_stack = np.stack((tem6, tem7, tem8, tem6, tem7, tem8, tem6, tem7, tem8, tem6), axis=0)\n",
        "print(train_stack.shape)\n",
        "\n",
        "#saving the testing set on disk\n",
        "np.save(ROOT_PATH + '/data/test_data_3_10copy', train_stack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_np = np.stack(X_train)\n",
        "y_train_np = np.stack(y_train)\n",
        "X_val_np = np.stack(X_val)\n",
        "y_val_np = np.stack(y_val)\n",
        "X_test_np = np.stack(X_test)\n",
        "y_test_np = np.stack(y_test)\n",
        "\n",
        "print(X_test_np.shape)\n",
        "print(X_train_np.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pe8QLAVptZFG"
      },
      "outputs": [],
      "source": [
        "#Discriminator Model\n",
        "\n",
        "def define_discriminator(image_shape):\n",
        "  '''\n",
        "  This function portrays the discriminator model based on the image dimensions\n",
        "  args --> image_shape: (x,y) for 2d; (x,y,d) for 3d\n",
        "  '''\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "  in_src_image = Input(shape=image_shape)\n",
        "  # target image input\n",
        "  in_target_image = Input(shape=image_shape)\n",
        "\n",
        "  merged = Concatenate()([in_src_image, in_target_image])\n",
        "\n",
        "  # C64\n",
        "  d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "\n",
        "  # C128\n",
        "  d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  \n",
        "  # C256\n",
        "  d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  \n",
        "  # C512\n",
        "  d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "  d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "  d = BatchNormalization()(d)\n",
        "  d = LeakyReLU(alpha=0.2)(d)\n",
        "\n",
        "  # patch output\n",
        "  d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "  patch_out = Activation('sigmoid')(d)\n",
        "  \n",
        "  # define model\n",
        "  model = Model([in_src_image, in_target_image], patch_out)\n",
        "  \n",
        "  # compile model\n",
        "  opt = Adam(lr=0.0004, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MsgP0qJNwqil"
      },
      "outputs": [],
      "source": [
        "#ENCODER BLOCK\n",
        "\n",
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
        "  '''\n",
        "  This function protrays the architecture of an encoder block\n",
        "  '''\n",
        "  # weight initialization\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "\n",
        "  #add downsampling layer\n",
        "  g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)  \t# add downsampling layer\n",
        "\n",
        "  # conditionally add batch normalization\n",
        "  if batchnorm:\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "\n",
        "  #Activating Leaky RelU\n",
        "  g = LeakyReLU(alpha=0.2)(g) \t\n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4N-1lG-1xSMh"
      },
      "outputs": [],
      "source": [
        "# DECODER BLOCK\n",
        "\n",
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
        "  '''\n",
        "  This function portrays the architecture of a decoder block\n",
        "  '''\n",
        "  #weight initialization\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "\n",
        "  #add upsampling layer\n",
        "  g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in) \n",
        "\n",
        "  #add batch normalization\n",
        "  g = BatchNormalization()(g, training=True)  \t\n",
        "\n",
        "  # conditionally add dropout\n",
        "  if dropout:\n",
        "    g = Dropout(0.5)(g, training=True)\n",
        "    \n",
        "  # merge with skip connection\n",
        "  #basically we concatenate the layers produced by (upconvolution) and the original layer in encoder block\n",
        "  g = Concatenate()([g, skip_in]) \n",
        "\n",
        "  # relu activation\n",
        "  g = Activation('relu')(g)\n",
        "  return g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XwI5jkWcyYoS"
      },
      "outputs": [],
      "source": [
        "#Generator Model\n",
        "\n",
        "def define_generator(image_shape=(256,256,3)):\n",
        "  '''\n",
        "  This function portrays the generator model based on the image dimensions\n",
        "  args --> image_shape: (x,y) for 2d; (x,y,d) for 3d\n",
        "  '''\n",
        "  # weight initialization\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "\n",
        "  # image input\n",
        "  in_image = Input(shape=image_shape)\n",
        "  \n",
        "  # encoder model\n",
        "  e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
        "  e2 = define_encoder_block(e1, 128)\n",
        "  e3 = define_encoder_block(e2, 256)\n",
        "  e4 = define_encoder_block(e3, 512)\n",
        "  e5 = define_encoder_block(e4, 512)\n",
        "  e6 = define_encoder_block(e5, 512)\n",
        "  e7 = define_encoder_block(e6, 512)\n",
        "  \n",
        "  # bottleneck, no batch norm and relu\n",
        "  b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
        "  b = Activation('relu')(b)\n",
        "  \n",
        "  # decoder model\n",
        "  d1 = decoder_block(b, e7, 512)\n",
        "  d2 = decoder_block(d1, e6, 512)\n",
        "  d3 = decoder_block(d2, e5, 512)\n",
        "  d4 = decoder_block(d3, e4, 512, dropout=False)\n",
        "  d5 = decoder_block(d4, e3, 256, dropout=False)\n",
        "  d6 = decoder_block(d5, e2, 128, dropout=False)\n",
        "  d7 = decoder_block(d6, e1, 64, dropout=False)\n",
        "  \n",
        "  # output\n",
        "  g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
        "  out_image = Activation('tanh')(g)\n",
        "  \n",
        "  # define model\n",
        "  model = Model(in_image, out_image)\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xSdJdr15zFh2"
      },
      "outputs": [],
      "source": [
        "# Combine generator and Discriminator Model\n",
        "def define_gan(g_model, d_model, image_shape):\n",
        "  '''\n",
        "  This function portrays the proposed GAN architecture\n",
        "  args--> generator model, discriminator model, image shape \n",
        "  '''\n",
        "  # make weights in the discriminator not trainable\n",
        "  d_model.trainable = False\n",
        "  \n",
        "  # define the source image\n",
        "  in_src = Input(shape=image_shape)\n",
        "  \n",
        "  # connect the source image to the generator input\n",
        "  gen_out = g_model(in_src)\n",
        "  \n",
        "  # connect the source input and generator output to the discriminator input\n",
        "  dis_out = d_model([in_src, gen_out])\n",
        "  \n",
        "  # src image as input, generated image and classification output\n",
        "  model = Model(in_src, [dis_out, gen_out])\n",
        "  \n",
        "  # compile model\n",
        "  opt = Adam(lr=0.0004, beta_1=0.5)\n",
        "  model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "L0FYl5iQ0z7V"
      },
      "outputs": [],
      "source": [
        "# select a batch of random samples, returns images and target\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "  '''\n",
        "  This functions selects a batch of random samples and returns an image with its target\n",
        "  args --> image_stack, number of samples, patch shape\n",
        "  '''\n",
        "  # unpack dataset\n",
        "  trainA, trainB = dataset\n",
        "  \n",
        "  # choose random instances\n",
        "  ix = randint(0, trainA.shape[0], n_samples)\n",
        "  \n",
        "  # retrieve selected images\n",
        "  X1, X2 = trainA[ix], trainB[ix]\n",
        "  \n",
        "  # generate 'real' class labels (1)\n",
        "  y = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "  \n",
        "  return [X1, X2], y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PqmRMxgT1JRy"
      },
      "outputs": [],
      "source": [
        "# generate a batch of images, returns images and targets\n",
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "  '''\n",
        "  This function uses the generator model to generate fake samples \n",
        "  args --> generator model, number of samples, patch shape\n",
        "  '''\n",
        "  # generate fake instance\n",
        "  X = g_model.predict(samples)\n",
        "  # create 'fake' class labels (0)\n",
        "  y = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MBsfgaXZ2dBO"
      },
      "outputs": [],
      "source": [
        "#Epoch operations to summarize model performance\n",
        "\n",
        "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
        "  '''\n",
        "  This function generates sample to pass on to the GAN, and creates the plot at 100X epochs with the src, tar and generated image\n",
        "  args --> epoch step, generator model, dataset, number of samples\n",
        "  '''\n",
        "  # select a sample of input images\n",
        "  [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "  \n",
        "  # generate a batch of fake samples\n",
        "  X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "  \n",
        "  # scale all pixels from [-1,1] to [0,1]\n",
        "  X_realA = (X_realA + 1) / 2.0\n",
        "  X_realB = (X_realB + 1) / 2.0\n",
        "  X_fakeB = (X_fakeB + 1) / 2.0\n",
        "  \n",
        "  # plot real source images\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_realA[i])\n",
        "  \n",
        "  # plot generated target image\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_fakeB[i])\n",
        "  \n",
        "  # plot real target image\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_realB[i])\n",
        "  \n",
        "  # save plot to file\n",
        "  filename1 = 'plot_%06d.png' % (step+1)\n",
        "  pyplot.savefig(filename1)\n",
        "  pyplot.close()\n",
        "  \n",
        "  # save the generator model\n",
        "  filename2 = 'model_%06d.h5' % (step+1)\n",
        "  g_model.save(filename2)\n",
        "  print('>Saved: %s and %s' % (filename1, filename2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0TqpaIc8JDgk"
      },
      "outputs": [],
      "source": [
        "def train(d_model, g_model, gan_model, dataset, n_epochs=200, n_batch=1):\n",
        "  '''\n",
        "  This function is for training the model based on the pixel-to-pixel architecture to achieve image to image tramslation using GAM\n",
        "  args --> discriminator model, generator model, GAN model, train/test set, number of epochs, number of train batches/batch size\n",
        "  '''\n",
        "  # determine the output square shape of the discriminator\n",
        "  n_patch = d_model.output_shape[1]\n",
        "  \n",
        "  # unpack dataset\n",
        "  trainA, trainB = dataset\n",
        "  \n",
        "  # calculate the number of batches per training epoch\n",
        "  bat_per_epo = int(len(trainA) / n_batch)\n",
        "  \n",
        "  # calculate the number of training iterations\n",
        "  n_steps = bat_per_epo * n_epochs\n",
        "  \n",
        "  #log all losses\n",
        "  d_loss1_log = []\n",
        "  d_loss2_log = []\n",
        "  g_loss_log = []\n",
        "\n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_steps):\n",
        "    # select a batch of real samples\n",
        "    [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "    \n",
        "    # generate a batch of fake samples\n",
        "    X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "    \n",
        "    # update discriminator for real samples\n",
        "    d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "    d_loss1_log.append(d_loss1)\n",
        "    \n",
        "    # update discriminator for generated samples\n",
        "    d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "    d_loss2_log.append(d_loss2)\n",
        "    \n",
        "    # update the generator\n",
        "    g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "    g_loss_log.append(g_loss)\n",
        "    \n",
        "    # summarize performance - plot loss per epoch\n",
        "    \n",
        "    plt.clf()\n",
        "    plt.figure(figsize=(20,12))\n",
        "    plt.title('Epoch:%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "    plt.xlabel('Epoch', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.plot(d_loss1_log, 'r-', lw=2, label='d_loss1')\n",
        "    plt.plot(d_loss2_log, 'b-', lw=1, label='d_loss2')\n",
        "    plt.plot(g_loss_log, 'g-', lw=1, label='g_loss')\n",
        "    plt.legend(prop={'size':16})\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())\n",
        "    print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "    \n",
        "    # summarize model performance\n",
        "    # if (i+1) % (bat_per_epo * 10) == 0:\n",
        "    if (i+1) in [100, 200]:\n",
        "      plt.savefig('./loss_graph_%06d.jpg' % (i+1), bbox_inches='tight')\n",
        "      summarize_performance(i, g_model, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "colab_type": "code",
        "id": "eo9Q44PjJxRQ",
        "outputId": "dcfa656d-981b-4909-b07b-8d945d90a556"
      },
      "outputs": [],
      "source": [
        "#Training Model\n",
        "dataset=(X_train_np,X_train_np)\n",
        "print('loaded - ', X_train_np.shape, X_train_np.shape)\n",
        "\n",
        "# define input shape based on the loaded dataset\n",
        "image_shape=X_train_np.shape[1:]\n",
        "\n",
        "# define the models\n",
        "d_model = define_discriminator(image_shape)\n",
        "g_model = define_generator(image_shape)\n",
        "\n",
        "# define the composite model\n",
        "gan_model = define_gan(g_model, d_model, image_shape)\n",
        "\n",
        "# train model\n",
        "train(d_model, g_model, gan_model, dataset, n_epochs=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w_6Lq0wgRTvn"
      },
      "source": [
        "##Step 7: Predicting the Cloud Free Image\n",
        "\n",
        "In the previouos step, we have trained the model for predef number of epochs and predrf batch size. The model at each 100 epoch has been saved on the disk. From this point on, we can choose not to execute the previous steps as we now hace the option of loding the model straight from the disk and making the prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PIGvh2mlSN8B"
      },
      "source": [
        "> 7.1 Importing packages to have seperate implementation from this step"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Exeq6G4zoUqQ",
        "PliCBvplq-hh",
        "SEW0teLIpDgb",
        "Kml8rBJeslua"
      ],
      "include_colab_link": true,
      "name": "Cloud_Removal_GANs.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "dea95fc5b86fbf6522516b8c46d4a24409d5c3ef837fd6ef0163978ea2b0251f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
